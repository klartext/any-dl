ENHANCEMENT:
============




BUGs:
=====


 - using linkextract on "file:///"-urls does not work.

 - "http://www.ardmediathek.de/das-erste/menschen-bei-maischberger/(...)"
   lÃ¤sst sich mit -i -p mp4
              und anderen Parsern, evbenfalls mit -i
              downloaden. Aber ohne -i nicht!




BETTER STRUCTURE, CLEANER CODE:
===============================

 - tagselect: unparse a table should be possible...

 - URL-analysis in network.ml: wouldn#t it be better in parsers.ml?
 - maybe remove print_string(), because print() does the same now.
 - dump is rather a deparse; dump_data is dump-like or deparse-like too;
   possibly better names should be used here



FEATUREs:
=========

 a bit of priority
 =================

 - tagselect: argpairs: different deparsing scheme also would make sense.
              maybe giving back matrix instead of array/list?

 - tagselect / csv_save: things like tagselect( ... | argpairs)
                         would make sense to be converted to a table of the form

                           argname_1     argname_2  argname_3 ...
                             argval_1_1  argval_2_1   argval_3_1
                             argval_1_2  argval_2_2   argval_3_2
                             argval_1_3  argval_2_3   argval_3_3
                             argval_1_4  argval_2_4   argval_3_4
                              ...         ...          ...

                            and then saved with csv_save...
                       So, this scheme would make sense.
                       But the scheme in use now also make sense to get a good and fast overview.
                       So, maybe, and option for selecting the deparsing scheme would make sense.


 - iselectmatch with 3 parameters would be good:
     * selection-source
     * selection-match-string
     * default-selection

   At the moment, selection-source and default-selection are both the same!


 - tagselect: regexp's for selection-pattern  (extraction-pattern also?)

 - converting HTML to named variables (like it also was planned for XML)
   This would make it easier to select certain data from a docoment / doclist.

 - tagselect: allow Doclist argument as tmpvar to work on

 - DropCol / DropRow also for other data-structures, like Doc_array, Url_list and so on.



 - Special variable for the  tmpvar? maybe $TMPVAR ?

  - json-files for saving data with some structure?

 - possibly an option/pragma, that allows parsers to be *not* invoked
   via "-a" CLI switch, because some parsers download anything, not
   a certain video-file (because they are more general, like general
   parsers that download all pics or so.)


 - Logfile for sucessful and failed downloads

 - UNIQ for URLs (e.g. in case, linkextract gives certain URLs multpiple times)

 - if/then/else?  try/catch?

 - possibly a true stack ( push, pop, dup, exch, ...) might make sense,
   enhancing the one-val-stack concept.

 - * new built-in needed:  storematch("MyName"); # stores match.-array under   MyName.(col).(row)
   * new command: astore("NAME");  (autostore() -> store current value like matrix -> NAME.(x).(y) or something like that
   * syntacttic sugar: access to Array.items via  Dot-notation... eg.  $STACK.0, $STACK.1, $STACK.2 and so on.

 - call/use command, to call a different parser? ==>   parsername "parsefoobar": ( "url" ) begin call "ard_mediathek_get" end
   This way, not only an URL can be added, but also a name, or a distinct parser that just calls another parser.
   Not checked so far, how much effort for implementing this would be.
   But it looks reasonable to invest that effort for this feature.

 - Testsuite would help a lot (example where helpful: linkextract problem in  commit c7f3c1ec98d9c3e88d47c93637a50879c0711d05 )
     - tesing sites as qell as testing parsers would be needed.

 lesser Priority
 ===============

  - javascript engine

 - !!!! XML-deparse -> get values from XML via name :-)

 - matches and assignments -> each match-group one varname... like   (url,vidname,foobar) = match("^(.*)xyz(lala).*?(foobar)")

 - maybe add a more sophisticated program invocation (fork/exec or popen) ?

 - would it make sense to allow get also to read the streams?
   Or should there be some other special commadn for it?
   like "get_stream"?
   (but how to achieve this? by using fork/exec to the tools used via system() at the moment?
   Or using stream-reading libraries for this? Are there any such libs and/or OCaml bindings?)

 - possibly switch to ulex instead of ocamllex:
     http://alain.frisch.fr/soft.html#ulex
     http://caml.inria.fr/pub/ml-archives/caml-list/2006/04/6d31ef03a5a1f9a182a9ed2422d266a4.en.html
    ... but ocamlnet uses ulex internally? I could just use ocamlnet for reading/parsing files maybe?

 - other syntax/grammar:
     - parser-Definitions only what to do, URLs not (or not necessarily?) at parser-definition.
     - command for attaching parsers to URLs or vice versa.
       something like:     attach parser "ard_mediathek_get" to ("http://...", "http://"...")
                 or        attach_urls "ard_mediathek_get" ("http://...", "http://"...")





QUESTIONS:
==========

 - Parsername as predefined named Variable also?  ( $PARSERNAME ?? ) 

 - how detailed/verbose or how non-verbose should the commands be?
   e.g.  should print; print onl ythe url or url and referrer?
   Should there be more specific commands?
   Or should print get parameters?
   Or should there be something like
      print with referrer;

 - Implementing macros or not?

 - the interactive loop for menue-item selection catches failing int_of_string: default-pattern is used.
   It could also ask again for correct selection.

